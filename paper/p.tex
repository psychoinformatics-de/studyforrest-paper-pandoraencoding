\input{boilerplate}
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }


\begin{document}
\bibliographystyle{unsrtnat}
%\title{Exploring the role of auxilliary parameters in evaluating voxel-wise encoding models}
\title{The role of auxilliary parameters in evaluating voxel-wise encoding models for 3T and 7T BOLD fMRI data}


\author[1]{Moritz~Boos}
\author[2]{J.~Swaroop~Guntupalli}
\author[1]{Jochem~W.~Rieger}
\author[3,4]{Michael~Hanke}

\affil[1]{Applied Neurocognitive Psychology Lab and Cluster of Excellence "Hearing4all", University of Oldenburg, Oldenburg, Germany}
\affil[2]{Department of Psychological and Brain Sciences,
  Dartmouth College, Hanover, New Hampshire, USA}
\affil[3]{Institute of Neuroscience and Medicine, Brain \& Behaviour (INM-7), Research Centre Jülich, Jülich, Germany}
\affil[4]{Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University Düsseldorf, Düsseldorf, Germany}
\maketitle

\begin{abstract}
In neuroimaging, voxel-wise encoding models are a popular tool to predict brain activity elicited by a stimulus.
To evaluate the accuracy of these predictions across multiple voxels, one can choose between multiple quality metrics.
However, each quality metric requires specifying auxilliary parameters such as the number and selection criteria of voxels, whose influence on model validation is unknown.
In this study, we systematically vary these parameters and observe their effects on three common quality metrics of voxel-wise encoding models in two open datasets of 3- and 7-Tesla BOLD fMRI activity elicited by musical stimuli.
We show that such auxilliary parameters not only exert substantial influence on model validation, but also differ in how they affect each quality metric.
Finally, we give several recommendations for validating voxel-wise encoding models that may limit variability due to different numbers of voxels, voxel selection criteria, and magnetic field strengths.
\end{abstract}

\clearpage


\section*{Introduction}

In functional magnetic resonance imaging (f{MRI}) research, voxel-wise encoding models are an increasingly popular tool to characterize the relationship between a real world stimulus and BOLD activity patterns \citep{TD+06,KG+08,NG11,huth2012continuous,holdgraf2016rapid}. However, researchers face many degrees of freedom in constructing voxel-wise encoding models, such as how to represent the stimulus or how to estimate an accurate model. To evaluate the fidelity of these choices and the resulting voxel-wise encoding models, multiple quality metrics exist that assess how well these models approximate spatial patterns of brain activity across multiple voxels. \citet{ML08} first introduced binary retrieval accuracy --- a measure that assesses whether an encoding model's predicted spatial activation patterns can, on average, identify the correct stimulus against a decoy --- to evaluate the predicted f{MRI} images for the meaning of nouns. \citet{KG+08} used stimulus identification accuracy, and \citet{NG09} used stimulus reconstruction quality as a metric for encoding performance. In auditory neuroscience, only binary retrieval accuracy \citep{CTK+2012,hoefle2018identifying} and stimulus identification \citep{SF14,allen2018encoding} have been used.
However, while providing novel insights into the cortical processing of sensory stimuli, it is unknown how additional parameters of the data analysis that are unrelated to the encoding models themselves affect these quality metrics. Hence, it remains difficult to compare the quality of voxel-wise encoding models when they differ in such auxilliary parameters.
In this study, we aim to tackle this problem by documenting the effect of two common auxilliary parameters in the validation of voxel-wise encoding models --- the number of voxels used and how they were selected --- on different quality metrics for low and high field strength (3 and 7-Tesla f{MRI}). To this end, we utilize a 3T f{MRI} study on the perception of musical genres \citep{CTK+2012} which has recently been replicated in 7T \citep{HDH+2015}. Using these datasets, we apply three approaches to encoding model validation, --- binary retrieval accuracy \citep{ML08}, stimulus identification \citep{KG+08,SF14} and decoding accuracy of the stimulus \citep{NG09} ---, and compare them in a 3T and 7T dataset with identical stimuli and comparable design, for different choices in data analysis parameters.

\section*{Methods}

\subsection*{Stimuli}

Stimuli were five natural, stereo, high-quality music stimuli (\unit[6]{s} duration; \unit[44.1]{kHz} sampling rate) for each of five different musical genres: 1) Ambient, 2) Roots Country 3) Heavy Metal, 4) 50s Rock'n'Roll, and 5) Symphonic. Previously, all 25 stimuli have been made publicly available \citep{HDH+2015}.

\subsection*{f{MRI} data}

The analyses presented here were performed on two independently recorded, and previously published datasets \citep{CTK+2012,HDH+2015}. While these datasets have been acquired using identical stimuli, with the same number of acquisition runs and number of stimulation trials, they nevertheless differ in their precise stimulation timing, stimulation setup and equipment, as well as other acquisition details. A brief description of both datasets is provided below. For more information the reader is referred to the respective publications.

\paragraph{\unit[3]{Tesla}}
%
Participants were scanned in a Philips Intera Achieva scanner with 32 channel SENSE head coil at the Center for Cognitive Neuroscience at Dartmouth College. Functional scans were acquired with an echo planar imaging sequence (\unit[2]{s} TR; \unit[35]{ms} TR, \unit[90]{\textdegree} flip angle) with \unit[3]{mm} isotropic voxels.
% MIH: as it was published before we probably don't have to include this
%All participants consented in accordance with the procedures set by the Committee
%for the Protection of Human Subjects at the Dartmouth College. 
Each participant participated in eight functional runs. Stimuli were presented in an event-related design with a variable trial duration. Each run consisted of a total of 29 trials corresponding to 25 music clips and 4 catch trials presented randomly during each run. Each trial started with a \unit[6]{s} music clip followed by \unit[4-8]{s} of fixation. For catch trials, a question appeared after the audio presentation asking whether an acoustic feature is present in the music clip such as vocals, guitar, etc. Subjects responded “Yes” or “No” with a button box. Catch trials were supposed to help keep the participants’ attention to the music and were discarded from the analyses. Each run had \unit[4]{s} of fixation at the beginning and \unit[10]{s} of fixation at the end. For further details, see \citet{CTK+2012}

\paragraph{\sevenT}
%
The procedures for the \unit[7]{Tesla} acquisition were highly similar and only
criticial differences are reported here. Echo-planar BOLD images (gradient-echo, \unit[2]{s} repetition time (TR), \unit[22]{ms} echo time, \unit[0.78]{ms} echo spacing, GRAPPA acceleration factor 3) were acquired using a whole-body \sevenT\ Siemens MAGNETOM magnetic resonance scanner equipped with a 32 channel brain receive coil. 36 axial slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane resolution) with a 10\% inter-slice gap were recorded in ascending order.  Slices were oriented to include the ventral portions of frontal and occipital cortex while minimizing intersection with the eyeballs. The field-of-view was centred on the approximate location of Heschl's gyrus along the rostral-caudal axis.

Instead of dedicated catch trials, similar catch questions as for the 3T acquisition were presented \unit[4]{s} after the end of the stimulus in trials with an \unit[8]{s} inter-stimulus delay. Consequently, each run consisted of 25 trials, and no trials were discarded from the analysis.  There was no additional fixation at the start of a run. For further details, see \citet{HDH+2015}, and \citet{HBI+14} for details on MRI acquisition methods.

\subsection*{Preprocessing}

Approximate temporal lobe masks for each participant were extracted from Montreal Neurological Institute coordinate space using FSL \citep{SJB+04,JBB+12}, and projected into the subject-specific coordinate system.  Each voxel inside the temporal lobe mask was run-wise $Z$-scored and linearly de-trended using PyMVPA \citep{HHS09b}. After preprocessing, 3T f{MRI} data consisted of 16999 voxels and 7T f{MRI} data of 115439 voxels.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/encoding_scheme}

  \caption{A schematic overview of the encoding process. The spectrogram for
    each stimulus is transformed into its low-quefrency mel-frequency spectrum
    (LQ-MFS). Then, the encoding features are extracted by a sliding window
    from the LQ-MFS. Using these features, encoding model is trained on all
    runs in the training set, and used to predict the BOLD activity of the
  left-out run.  These predictions are subsequently used for validation.}

 \label{fig:encoding_scheme}
\end{figure}

\subsection*{Encoding model}

To build an encoding model with high predictive power, one needs to find an appropriate feature representation of the music stimuli \citep{holdgraf2017encoding}. \citet{CTK+2012} compared different feature representations of the same stimuli in the 3T dataset. They found features corresponding to the timbre of the stimulus offer the best discriminative power. We chose a similar feature set, related to timbre, that was made available by \citet{HDH+2015}, the low-quefrency mel-frequency spectrum (LQ-MFS) of the stimulus. This stimulus representation consists of 48 LQ-MFS coefficients for each 100 ms segment of a stimulus, thus each stimulus overall comprises 2880 LQ-MFS coefficients.

For each f{MRI} sample $y_{vt}$ (where $t=1,2,,\ldots,T$ denotes the time-points, $T$ the number of samples in each run, and $v=1,2,\ldots,V$ denotes the voxels) the LQ-MFS features $x_{t}$ $[1\times\widetilde{M}]$ (where $\widetilde{M}$ is the number of LQ-MFS coefficients) of the corresponding two second part of the stimulus -- immediately prior to the acquision time point -- were computed. In case there was no stimulus presented at time-point $t$, a zero vector $[1\times\widetilde{M}]$ was used. 

As the BOLD response is delayed,  the most recent feature vector was removed for each f{MRI} sample (this corresponds to an assumed \unit[2]{s} stimulus-response delay), and the new feature vector at time-point $t$ was created by concatenating the prior feature vectors $x_{t-1}$,$x_{t-2}$ and $x_{t-3}$ (see Figure \ref{fig:encoding_scheme}). From now on, we denote this stacked feature vector as $x_{t}$.  Feature vectors (and the corresponding f{MRI} sample) were removed from the analysis, if two-thirds or more of the concatenated feature vectors were zero-vectors.

The BOLD activity time-series, as well as the feature time-series, were vertically stacked, resulting in a matrix of features $X$ $[N\times M]$ (where $N$ is the number of f{MRI} samples, and $M$ is number of LQ-MFS coefficients, with $M=3\widetilde{M}$) and a matrix of BOLD activity $Y$ $[N\times V]$ (where $V$ is the number of voxels and $N$ as above). This lagging of the stimulus allows us to train the encoding model to predict the f{MRI} time-series without explicitly modelling the BOLD response. The encoding model could then be expressed as the probability to observe the BOLD activity at time-point $t$ and voxel $v$:
\begin{equation}
  \label{eq:encmo}
  p(y_{vt}|x_{t}) = N(y_{vt};x_{t}\beta_{v},\sigma)
\end{equation}
where $N(y;\mu,\sigma)$ denotes the probability density at $y$ for a
Gaussian with mean $\mu$ and standard deviation $\sigma$, and $\beta_{v}$ is a
$[M\times1]$ vector of regression coefficients specific to voxel $v$. The corresponding matrix vector equation for all voxels is
\begin{equation}
    \label{eq:encmat}
    \mathbf{Y} = \mathbf{X}\mathbf{\beta}
\end{equation}
where $\mathbf{Y}$ and $\mathbf{X}$ are defined as above and $\mathbf{\beta}$ is a $[M \times V]$ matrix of regression coefficients per voxel.
To reduce over-fitting, the regression-coefficients were estimated using ridge regression \citep{HK70}. Independently for each voxel, the regularization parameter $\lambda$ with the lowest mean squared error in a generalized leave-one-out cross-validation \citep{GHW79} was chosen from a set of candidate values. This set was chosen so that the highest and lowest values of $\lambda$ were only rarely selected in cross-validation. While this regularization parameter can exert a large influence on regression coefficients, and hence interpretation, we do not scrutinize its role here, since we are interested in choosing the amount of regularization that maximizes prediction accuracy.
All analyses were implemented using custom code that heavily utilizes the SciPy ecosystem \citep{jones2014scipy}, Scikit-Learn \citep{PVG+11}, Nilearn \citep{abraham2014machine}, and PyMVPA \citep{HHS09b}.

\subsection*{Quality metrics} 

Binary retrieval accuracy and the matching score quantify encoding model performance based on the difference between predicted and observed spatial patterns that stretch across multiple voxels. They assume that each stimulus is associated with one spatial patterns, and thus do not incorporate stimulus timecourses with multiple BOLD samples. Since our stimuli encompass three BOLD samples each, we treat each BOLD sample as as an individual stimulus in both the binary retrieval accuracy and the matching score. However, we need to avoid misclassifications in the case in which predicted BOLD activity for one sample of the stimulus is closest to another sample in the same stimulus -- and thus "misclassifies" the time point, but correctly classifies the stimulus. To deal with this issue, when testing each predicted BOLD sample we temporarily remove all other samples of the same stimulus from the validation set.
Hence, for each BOLD sample of a stimulus we test against $N = 25\:\times 3 - 2$ other stimuli per run (25 stimuli with 3 BOLD samples each).
Since all quality metrics are computed on the validation set in a leave-one-run-out cross-validation scheme, this leads to different number of samples used for the binary retrieval accuracy and machting score (number of samples $N$ = 3 samples $\times$ 25 stimuli $\times$ 8 runs = 600), and decoding accuracy ($N$ = 25 stimuli $\times$ 8 runs = 200 or $N$ = 5 genres $\times$ 8 runs = 40).

\subsubsection*{Binary retrieval accuracy}
%
Binary retrieval accuracy \citep{ML08} tests if an encoding model's predictions can, on average, differentiate a stimulus' observed BOLD activity from the BOLD activity of a decoy stimulus. Specifically, a stimulus pair is counted as succesfully classified, if the cosine similarity between predicted and observed f{MRI} responses is greater for the correctly matched predictions and observations than for the incorrectly matched ones (i.e. the similarity of the observed response of stimulus A with the predictions for stimulus B and vice versa). In our case, the binary retrieval measure for one predicted BOLD sample of a stimulus was computed by counting the correct matches for all (exhaustive) pair-wise combinations of all BOLD samples of all stimuli, independent of genre, while excluding other samples of the same stimulus, and then dividing by the number of combinations. This quantity was averaged across all samples of all stimuli.

\subsubsection*{Matching score}
%
An alternative measure of encoding performance is the correlation rank score or matching score \citep{SF14}. For each BOLD sample of a given stimulus $l_{n}$ in the validation set, its predicted BOLD activity $\widetilde{y}_{n}$ is correlated with the observed BOLD activity of every stimulus, $y_{n}$.
These correlations are then ordered, and the  matching score $m(l_{n})$ is \[m(l_{n}) = 1-\frac{rank(l_{n})-1}{N-1} \] Where $rank(l_{n})$ is the rank of the correlation between predicted $\widetilde{y}_{n}$ and observed $y_{n}$ BOLD activity of $l_{n}$, and $N$ = 25 stimuli $\times$ 3 samples - 2 = 73 is the number of BOLD samples in the validation set excluding the two other BOLD samples from the same stimulus.
Finally, the matching scores for all stimuli in the validation set are averaged.

\begin{figure}
  \centering
  \includegraphics[width=.8\linewidth]{pics/decoding_scheme.pdf}

  \caption{A schematic overview of the decoding of stimulus identity and stimulus music category. The
    predicted f{MRI} time series of the validation run is reduced in
    dimensionality by principal component analysis, and a multivariate-normal
    likelihood function $p(y_{t}|x_{t})$ is constructed.  From there, the
  probability distribution over music stimuli and subsequently musical
categories is estimated.}

 \label{fig:decoding_scheme}
\end{figure}


\subsubsection*{Decoding}

Instead of testing the encoding performance, we can also test the performance of a decoder based on the individual encoding models \citep{NG11} (Figure \ref{fig:decoding_scheme}). Our goal is to obtain the probability of which music stimulus was presented from the predicted spatial activation pattern across voxels. To go from the probability of a voxel activation given stimulus features $p(y_{vt}|x_{t})$ to probability of stimulus features given voxel activation $p(x_{t}|y_{t})$ we follow \citet{NG09} and separate our decoding scheme into two parts. 

\paragraph{Single- to Multi-Voxel Encoding}

First we condense the large number of voxel-specific encoding models into one multi-voxel encoding model.  To do this we project the predicted and observed f{MRI} data onto the first $k$ principal components of the $[N\times V]$ matrix of predicted BOLD activity. We choose $k$ via cross-validation to estimate the number of principal components that maximize the decoding accuracy on the training set for each participant. As in \citet{NG09} we construct the $k$-dimensional multivariate normal probability density function $p(y_{t}|x_{t})$ to obtain a likelihood function across voxels. 

\paragraph{Multi-Voxel Encoding to Decoding}

We now express this likelihood in terms of the label of the music stimulus, instead of its LQ-MFS features. We use the simplifying assumption that the BOLD activity is influenced by the music stimuli only through their LQ-MFS coefficients $x$, and --- given that each music stimulus was associated with only three (lagged) LQ-MFS representations --- the likelihood to observe a given triple of consecutive $y$ for a specific music stimulus $l_{n}$ is $p(y|l_{n}) \propto p(y_{t-1}|x_{1})p(y_{t}|x_{2})p(y_{t+1}|x_{3})$ where $t$ is the sample \unit[6]{seconds} after the start of the music stimulus, $x$ are the three LQ-MFS feature vectors of this stimulus, and $n=1..25$ denotes the stimulus. For a given triple of consecutive BOLD activity $y$ from the same stimulus, we can now estimate the probability distribution over music stimuli $p(l_{n}|y)$  by using Bayes' rule: $p(l_{n}|y) \propto p(y|l_{n})p(l_{n})$. Since each stimulus was presented exactly once, it has an uniform prior distribution with $p(l_{n})=\frac{1}{25}$. The mode of this distribution is the most probable presented stimulus given the data.
Additionally we decode the musical genre of the presented stimulus given the observed BOLD activity as the mode of $p(c_{n}|y) = \sum\nolimits_{l \in  stim(c_{n})} p(l|y)$ where $stim(c_{n})$ are the labels of the stimuli   belonging to the genre $c_{n}$.

\subsection*{Voxel selection}

We varied the number of voxels used in the analysis, both for 3T and 7T f{MRI} data, and selected which voxels to keep by two different criteria. Both criteria were based on voxel characteristics in the training set only.

\paragraph{Selection by stability}

\citet{ML08} selected the 500 most stable voxels for their analysis. For a single voxel, each run can be represented as a vector of BOLD activity, where each entry is associated with one stimulus. A voxel's "stability score" is then the average (pair-wise) correlation between the vectors of the eight runs for all combinations of runs. Hence, this criterion selects voxels with consistent activation for each stimulus across runs.

\paragraph{Selection by $r^2$}

As we are interested in encoding performance, we can use the quality of predictions of each voxel's encoding model as a selection criterion. We compute the coefficient of determination $r^2$ for each voxel-specific encoding model in the training set. Using this criterion selects voxels whose activity can be explained best by an LQ-MFS-based encoding model.

\section*{Results}

We trained voxel-wise encoding models to predict voxel activity using a LQ-MFS representation of music stimuli in 3T and 7T f{MRI} data.
First, we evaluate each voxel's encoding model individually. Figure \ref{fig:r2_plot} shows $r^{2}$ values from an eight fold leave-one-run-out cross-validation of the 10000 voxels with highest $r^{2}$ averaged across participants. While voxel-wise encoding models show higher peak $r^{2}$ in 3T than in 7T in the best performing models (left side of the figure), voxel-wise encoding models show higher $r^{2}$ in 7T than in 3T for all other voxels.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/r2_plot.pdf}
	
    \caption{Voxel-wise $r^{2}$ between predicted and observed BOLD activity for each left-out run, averaged across participants and runs with 95\% confidence interval. Shown here are the 10000 voxels with highest $r^{2}$ for 3T and 7T.
  }

 \label{fig:r2_plot}\end{figure}

Next, we evaluate sets of voxel-wise encoding models for 3T and 7T using several quality metrics. We varied the number of voxels used in the analysis and how they were selected, both for 3T and 7T f{MRI} data, and compared the resulting differences in three quality metrics of encoding models. To differentiate between effects of voxel number and overall volume of the voxels, which differs in 3T and 7T, we show the results as a function of the number of voxels, as well as overall volume of these voxels.

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{pics/binary.pdf}
	
  \caption{\textbf{A} Mean binary retrieval accuracy as a function of the
  included number of voxels for 3T and 7T, for stability- and $r^2$-based
  voxel selection. Error bars denote the bootstrapped 95\% confidence interval
  of the mean. The mean is taken over binary retrieval accuracies of eight runs
  for each of the 19 participants. \textbf{B} Mean binary retrieval accuracy as
a function of the overall volume of the included voxels for 3T and 7T, for
stability- and $r^2$-based voxel selection.
}

 \label{fig:binary_retrieval}\end{figure*}

\subsection*{Binary retrieval accuracy}

Figure \ref{fig:binary_retrieval} shows differences in binary retrieval accuracy for different numbers of voxels, selection strategies, and field strength.
For both 3T and 7T data, and both selection strategies, the maximum binary retrieval accuracy is achieved with relatively low number of voxels (3T and $r^{2}$: \unit[250]{voxels}, \unit[6.75]{$cm^{3}$} volume, 7T and $r^{2}$: \unit[500]{voxels}, \unit[0.68]{$cm^{3}$} volume, 3T and stability selection: \unit[500]{voxels}, \unit[13.5]{$cm^{3}$} volume, 7T and stability selection: \unit[1000]{voxels}, \unit[2.7]{$cm^{3}$} volume) and decreases the more voxels are included. For both stability selection criteria, 3T outperforms or is equal to 7T across all numbers of voxels. With regard to the voxel selection strategies, selection by $r^{2}$ outperforms or is equal to selection by stability across all numbers of voxels (Figure \ref{fig:binary_retrieval_selection}).

\begin{figure*}
  \centering
    \includegraphics[width=\linewidth]{pics/rank.pdf}
	
  \caption{\textbf{A} Mean matching rank score as a function of the included number
  of voxels for 3T and 7T, for stability- and $r^2$-based voxel selection.
  Error bars denote the bootstrapped 95\% confidence interval of the mean. The
  mean is taken over binary retrieval accuracies of eight runs for each of the
  19 participants. \textbf{B} Mean matching rank score as a function of the overall
volume of the included voxels for 3T and 7T, for stability- and
$r^2$-based voxel selection.}

 \label{fig:matching_score}
\end{figure*}

\subsection*{Matching score}

Figure \ref{fig:matching_score} shows differences in matching score for different numbers of voxels, selection strategies, and field strength. Matching scores in both 3T and 7T data peak at high numbers of voxels (3T and $r^{2}$: \unit[1000]{voxels}, \unit[27]{$cm^{3}$} volume, 7T and $r^{2}$: \unit[5000]{voxels}, \unit[13.5]{$cm^{3}$} volume, 3T and stability selection: \unit[500]{voxels}, \unit[13.5]{$cm^{3}$} volume, 7T and stability selection: \unit[5000]{voxels}, \unit[13.5]{$cm^{3}$} volume). Indexing by the overall volumes these voxels encompass reveals that the matching score peaks for 3T and 7T at (for stability selection) or close to (for selection by $r^{2}$) the same volume. For both stability selection and selection by $r^{2}$, 3T outperforms 7T for the same number of voxels, except for the largest two numbers of voxels, and --- in terms of the volume the voxels take --- 7T outperforms 3T for volumes up to \unit[2.7]{$cm^{3}$} and 3T outperforms 7T for volumes beyond \unit[27]{$cm^{3}$}. Furthermore, for 3T data, selection by stability and selection by $r^{2}$ perform similarly, while selection by $r^{2}$ outperforms selection by stability consistently in 7T data (Figure \ref{fig:matching_score_selection}).

\begin{figure*}
  \centering
    \includegraphics[width=\linewidth]{pics/decoding.pdf}

	\caption{\textbf{A} Mean decoding accuracy of individual music stimuli as a function of
  the included number of voxels for 3T and 7T, for stability- and
  $r^2$-based voxel selection. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over decoding
  accuracies of eight runs for each of the 19 participants. Chance level is
    0.04. \textbf{B} Mean
decoding accuracy of individual music stimuli as a function of the overall volume of the
included voxels for 3T and 7T, for stability- and $r^2$-based voxel
selection. Chance level is 0.04.
}
 \label{fig:decoding_accuracy_stimulus}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{pics/decoding_genre.pdf}

  \caption{\textbf{A} Mean decoding accuracy of music category as a function of
  the included number of voxels for 3T and 7T, for stability- and
  $r^2$-based voxel selection. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over decoding
  accuracies of eight runs for each of the 19 participants. Chance level is 0.2. \textbf{B} Mean
decoding accuracy of music category as a function of the overall volume of the
included voxels for 3T and 7T, for stability- and $r^2$-based voxel
selection. Chance level is 0.2.
}

 \label{fig:decoding_accuracy}
\end{figure*}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/conf_mats.pdf}

  \caption{ Confusion matrices for genre decoding, separated by field-strength and voxel selection. Each confusion matrix is averaged across the different number of voxels, participants and runs.}

 \label{fig:confusion_matrices}
\end{figure}

\subsection*{Decoding accuracy}

Figure \ref{fig:decoding_accuracy_stimulus} shows the accuracy of decoding each individual stimulus (chance level 0.04) for different numbers of voxels, selection strategies, and field strength. 3T consistently outperforms 7T in decoding individual stimuli: if voxels are selected by stability, 3T outperforms 7T for all numbers of voxels, if voxels are selected by $r^{2}$, 3T outperforms 7T for all but the two highest numbers of voxels. For 7T data, the number of voxels matters little, however, for 3T data, optimal decoding accuracy is reached at a relatively low number of voxels (\unit[250]{voxels} for selection by $r^{2}$ and selection by stability, \unit[6.75]{$cm^{3}$} volume), and subsequently decreases if more voxels are included.

Instead of decoding individual stimuli, we can also decode the musical genre of each individual stimulus. Figure \ref{fig:decoding_accuracy} shows the accuracy of decoding music genre (chance level 0.20) for different numbers of voxels, selection strategies, and field strength. Here a different pattern emerges: decoding accuracy is higher for 7T data than for 3T data, across all numbers of voxels and selection criteria. Potentially, this suggests that 7T BOLD activity contains different information than 3T.  For both selection strategies, 7T shows higher maximum genre decoding accuracy for high numbers of voxels, while 3T shows higher maximum genre decoding accuracy for lower numbers of voxels.

Next, we want to answer the question which musical genres can be best predicted, and if those differ between 3T and 7T data. For this we compute the confusion matrix from the out-of-sample predictions of all classifiers: Rows denote the stimulus' true genre, columns denote the predicted genre. Each cell contains the count -- or proportion -- of occurences of this combination of true and predicted genres. Maximum classification accuracy leads to a diagonal confusion matrix.

Figure \ref{fig:confusion_matrices} shows the confusion matrices averaged across numbers of voxels for different field strength and selection criteria. The occurences are normalized per row, and their mean proportion across numbers of voxels are indicated in each cell. All confusion matrices show a higher misclassification between genres that contain speech (country, rock'n'roll and heavy metal) and between instrumental genres (ambient and symphonic), while very few genres that contain speech are incorrectly classified as instrumental or vice versa. The increased genre decoding accuracy in 7T is thus due to a better discrimination between the two instrumental genres --- ambient and symphonic --- and between the three vocal genres --- country, rock'n'roll, and heavy metal.

\section*{Discussion}

Our results show that all three quality metrics vary with the number of voxels and voxel selection strategies in both low and high field strengths. We demonstrate that the patterns of this variation differ between quality metrics and that field strength, voxel selection, and voxel number all affect these metrics differently.

We believe these results emphasize the significance of an often under- appreciated aspect of computational models in neuroimaging: near arbitrary choices in the data analysis -- like the number of voxels used or how they are selected -- have a considerable impact on the performance evaluation of voxel-wise encoding models. Furthermore, the impact of these choices is inconsistent across the considered validation strategies. The effects of these parameters are rarely known a-priori, and each choice of a specific value can often be easily justified, yet this undisclosed flexibility leads to considerable researcher degrees-of-freedom \citep{SNS11,hong2019false} that will bias the comparison between different voxel-wise encoding models and hinder generalization of findings across individual studies.

In light of the substantial effects individual parameters exert on validation strategies, it does not surprise that one of our results -- a higher matching score in 7T than in 3T BOLD fMRI data -- constrasts a  previous study \citep{SF14}, which found higher matching scores for encoding natural sounds in 7T than in 3T, although using a different number of stimuli in the two conditions.

However, in spite of these variabilities, we can abstract several recommendations for applying and validating voxel-wise encoding models.

%r2 > stability selection
First, it is likely that selecting voxels by the quality of its encoding model, such as the $r^{2}$ score, will outperform a selection of voxels based on their stable response to the stimulus --- however, this difference diminishes for large numbers of voxels. One reason that this effect is especially prominent when only few voxels are included could be the quickly diminishing returns for higher number of voxels: voxels in which  an encoding model performs well are already included, and each additional voxel decreases the joint encoding performance. Interestingly, this pattern differs in the matching score, where performance in 7T is worse for the smallest number of voxels and increases when more voxels are included. In fact, this holds true for selection by both $r^{2}$ and a stability criterion, and thus differentiates the matching score from both binary retrieval accuracy, as well as decoding accuracy. This contrasts especially with binary retrieval accuracy, results from a previous study indicate that low numbers of voxels lead to high binary retrieval accuracy \citep{hoefle2018identifying}. Our results seem to corroborate this finding, suggesting that binary retrieval accuracy, in contrast to the matching score, is sensitive to the inclusion of worse performing voxels.

Second, the number of voxels used has a large effect on all quality metrics, and to compare two sets of voxel-wise encoding models even on the same stimuli, one has to control for them. For most quality metrics, this control is most easily exercised by fixing the number of voxels directly, however, in case of the matching score, it can be more meaningful to fix the overall volume the two sets of encoding models encompass.

Third, while higher field strength might lead to better individual encoding models, it might not improve their decoding accuracy. We show that voxel-wise encoding models in 3T f{MRI} data outperform voxel-wise encoding models in 7T f{MRI} data in all but one quality metric. This indicates that high field strength does not provide more information per se, since stimuli can't be differentiated better in general, in spite of the higher $r^{2}$ in most voxels in 7T. However, our key insight is, that higher field strength can decrease the decoding accuracy for some labels of the stimulus, i.e. the decoding of individual stimuli, yet increase the decoding accuracy for other labels, i.e the decoding of the musical genre of individual stimuli. Thus, one has to exercise caution, when choosing which label to decode from a set of encoding models. One reason for this, could be that training encoding models on stimulus features can make stimulus labels somewhat arbitrary. Two similar stimuli can lead to similar and accurate predictions of BOLD activity, and hence a higher $r^{2}$,  yet this similarity could decrease a quality measure that is based on the correct discrimination between individual stimuli.

While we believe that these three recommendations can guide neuroimaging researchers, to conclusively assess if these recommendations generalize, more data are needed, since the results in this study are based on only two f{MRI} datasets encompassing the same auditory stimuli recorded in 3- and 7T. This facilitates a comparison between different field strengths, yet, further work is needed to show that these conclusions hold beyond this set of stimuli, our chosen stimulus representation, and even beyond the auditory domain.

\section*{Author contributions}

MB performed the analysis and wrote the manuscript.
JSG contributed to the manuscript.
JWR contributed to the manuscript.
MH contributed to the manuscript.

\section*{Competing Interests}

No competing interests were disclosed.

\section*{Grant Information}

This research was, in part, supported by the German Federal Ministry of
Education and Research (BMBF) as part of a US-German collaboration in
computational neuroscience (CRCNS; awarded to James Haxby, Peter Ramadge, and
Michael Hanke), co-funded by the BMBF and the US National Science Foundation
(BMBF 01GQ1112; NSF 1129855).  Work on the data-sharing technology employed for
this research was supported by US-German CRCNS project awarded to
Yaroslav~O.~Halchenko and Michael~Hanke, co-funded by the BMBF and the US
National Science Foundation (BMBF 01GQ1411; NSF 1429999).  Michael Hanke was
supported by funds from the German federal state of Saxony-Anhalt, Project:
Center for Behavioral Brain Sciences.
Moritz Boos and Jochem W. Rieger were supported by funds from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy – EXC 2177/1 - Project ID 390895286.

\section*{Acknowledgements}
%This section should acknowledge anyone who contributed to the research or the
%article but who does not qualify as an author based on the criteria provided
%earlier (e.g. someone or an organisation that provided writing assistance).
%Please state how they contributed; authors should obtain permission to
%acknowledge from all those mentioned in the Acknowledgements section.  Please
%do not list grant funding in this section (this should be included in the
%Grant information section - See above).

We are grateful to Michael Casey and the musicians.
We are also thankful for Cristiano Micheli's helpful feedback on an earlier version of this manuscript.

\bibliography{references}

\beginsupplement
\newpage
\section*{Supplementary information} \label{supplemental}
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{pics/binary_selection.pdf}
	
  \caption{\textbf{A} Mean binary retrieval accuracy as a function of the
  included number of voxels for 3T and 7T, for stability- and $r^2$-based
  voxel selection. Error bars denote the bootstrapped 95\% confidence interval
  of the mean. The mean is taken over binary retrieval accuracies of eight runs
  for each of the 19 participants.
}

 \label{fig:binary_retrieval_selection}\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=\linewidth]{pics/rank_selection.pdf}
	
  \caption{\textbf{A} Mean matching rank score as a function of the included number
  of voxels for 3T and 7T, for stability- and $r^2$-based voxel selection.
  Error bars denote the bootstrapped 95\% confidence interval of the mean. The
  mean is taken over binary retrieval accuracies of eight runs for each of the
  19 participants.}

 \label{fig:matching_score_selection}
\end{figure}


\begin{figure}[H]
  \centering
    \includegraphics[width=\linewidth]{pics/decoding_selection.pdf}

	\caption{\textbf{A} Mean decoding accuracy of individual music stimuli as a function of
  the included number of voxels for 3T and 7T, for stability- and
  $r^2$-based voxel selection. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over decoding
  accuracies of eight runs for each of the 19 participants. Chance level is
    0.04.
}
 \label{fig:decoding_accuracy_stimulus_selection}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{pics/decoding_genre_selection.pdf}

  \caption{Mean decoding accuracy of music category as a function of
  the included number of voxels for 3T and 7T, for stability- and
  $r^2$-based voxel selection. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over decoding
  accuracies of eight runs for each of the 19 participants. Chance level is 0.2.}

 \label{fig:decoding_accuracy_selection}
\end{figure}



\end{document}

