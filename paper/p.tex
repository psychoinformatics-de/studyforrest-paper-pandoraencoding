\input{boilerplate}

\begin{document}
\bibliographystyle{unsrtnat}
\title{Decoding musical genre with an fMRI encoding model of musical
features -- a 3T/7T comparison}


\author[1]{Moritz~Boos}
\author[2]{J.~Swaroop~Guntupalli}
\author[1]{Cristiano Micheli}
\author[1]{Jochem Rieger}
\author[0]{\ldots}
\author[3,4]{Michael~Hanke}

\affil[1]{Oldenburg, Germany}
\affil[2]{Department of Psychological and Brain Sciences,
  Dartmouth College, Hanover, New Hampshire, USA}
\affil[3]{Psychoinformatics lab, Department of Psychology II, University of
Magdeburg, Magdeburg, Germany}
\affil[4]{Center for Behavioral Brain Sciences, Magdeburg, Germany}
\maketitle
\thispagestyle{fancy}

\listoftodos

\begin{abstract}
% Abstracts should be up to 300 words and provide a succinct summary of the
% article. Although the abstract should explain why the article might be
% interesting, care should be taken not to inappropriately over-emphasise the
% importance of the work described in the article. Citations should not be used
% in the abstract, and the use of abbreviations should be minimized.

\todo[inline]{write abstract}
\end{abstract}

\clearpage


\section*{Introduction}

In functional magnetic resonance imaging (f{MRI}) research, voxel-wise encoding
models are an increasingly popular tool to characterize the relationship
between a real world stimulus and BOLD activity patterns
\citep{NG11,TD+06,KG+08,SZ09}.  Yet not all encoding models are made alike, and
a researcher has many degrees of freedom in constructing one.  Parameters of
the data analysis, as well as parameters of BOLD-acquisition, like field
strength and resolution, might impact encoding performance \citep{KB07,FK12}.
Especially high-field f{MRI} (7-Tesla and higher), with its increased spatial
specificity \citep{THW+05,YU08}, could lead to better encoding models \citep{FK12}. 
\citet{SF14} offer some empirical evidence that an encoding model performs better on
7-Tesla than 3-Tesla data, although differing number of stimuli are a potential
confound. 

After collecting the data, more choices await -- how to preprocess, which
voxels to select and how to parameterize the encoding model -- and little is known about their
consequences. Even when we obtain an encoding model for each included voxel,
we can still choose between different ways of evaluating its performance.

Previous studies have introduced several measures for the quality of
an encoding model: \citet{ML08} used a binary retrieval task\todo{measure?} --- checking
whether an encoding model's predictions can be used to identify a stimulus from
a pair of stimuli --- to evaluate the predicted f{MRI} images for the meaning
of nouns. \citet{KG+08} used stimulus identification\todo{accuracy?}, and \citet{NG09} used
stimulus reconstruction\todo{quality?}, as a metric for encoding performance. In auditory
neuroscience, where encoding models are less frequent, only binary retrieval
accuracy \citep{CTK+2012} and stimulus identification \citep{SF14} have been
used. 

It remains an open question if these validation strategies are consistent
across different choices of analysis parameters and BOLD-acquisition schemes.

Recently, a 3-Tesla f{MRI} study on the perception of musical genres
\citep{CTK+2012} has been replicated with 7-Tesla data \citep{HDH+2015}, making
it possible to directly compare the effect of field strength on data analysis
parameters and validation strategies for encoding models. In this study, we
apply three different approaches to encoding validation --- binary retrieval
accuracy \citep{ML08}, stimulus identification \citep{KG+08,SF14} and the
decoding accuracy of a stimulus's music genre, and compare them in a 3-Tesla
and 7-Tesla dataset with identical stimuli and comparable design for different
analysis strategies.

\section*{Methods}

\subsection*{Stimuli}

Stimuli were five natural, stereo, high-quality music stimuli (\unit[6]{s}
duration; \unit[44.1]{kHz} sampling rate) for each of five different musical
genres: 1) Ambient, 2) Roots Country 3) Heavy Metal, 4) 50s Rock'n'Roll, and 5)
Symphonic. Previously, all 25 stimuli and have been made publicly available
\citep{HDH+2015}.

\subsection*{fMRI data}

The analyses presented here were performed on two independently recorded, and
previously published datasets \citep{CTK+2012,HDH+2015} . While these datasets
have been acquired using identical stimuli, with the same number of acquisition
runs and number of stimulation trials, they nevertheless differ in their
precise stimulation timing, stimulation setup and equipment, as well as other
acquisition details. A brief description of both datasets is provided below.
For more information the reader is referred to the respective publications.

\paragraph{\unit[3]{Tesla}}
%
Participants were scanned in a Philips Intera Achieva scanner with 32 channel
SENSE head coil at the Center for Cognitive Neuroscience at Dartmouth College.
Functional scans were acquired with an echo planar imaging sequence
(\unit[2]{s} TR; \unit[35]{ms} TR, \unit[90]{\textdegree} flip angle) with
\unit[3]{mm} isotropic voxels.
% MIH: as it was published before we probably don't have to include this
%All subjects consented in accordance with the procedures set by the Committee
%for the Protection of Human Subjects at the Dartmouth College. 

Each subject participated in eight functional runs. Stimuli were presented in
an event-related design with a variable trial duration. Each run consisted of a
total of 29 trials corresponding to 25 music clips and 4 catch trials presented
randomly during each run. Each trial started with a \unit[6]{s} of music clip
followed by \unit[4-8]{s} of fixation. For catch trials, a question appeared
after the audio presentation asking whether a particular feature is present in
the music clip such as vocals, guitar, etc. Subjects responded “Yes” or “No”
with a button box. Catch trials helped keep the subjects’ attention to the
music and were discarded from the analyses. Each run had \unit[4]{s} of
fixation at the beginning and \unit[10]{s} of fixation at the end. For further
details, see \citet{CTK+2012}

\paragraph{\sevenT}
%
The procedures for the \unit[7]{Tesla} acquisition were highly similar and only
criticial differences are reported here. Echo-planar BOLD images
(gradient-echo, \unit[2]{s} repetition time (TR), \unit[22]{ms} echo time,
\unit[0.78]{ms} echo spacing, GRAPPA acceleration factor 3) were acquired using
a whole-body \sevenT\ Siemens MAGNETOM magnetic resonance scanner equipped with
a 32 channel brain receive coil. 36 axial slices (thickness \unit[1.4]{mm},
\unit[1.4 $\times$ 1.4]{mm} in-plane resolution) with a 10\% inter-slice gap
were recorded in ascending order.  Slices were oriented to include the ventral
portions of frontal and occipital cortex while minimizing intersection with the
eyeballs. The field-of-view was centred on the approximate location of Heschl's
gyrus.

Instead of dedicated catch trials, similar catch questions as for the 3T
acquisition were presented \unit[4]{s} after the end of the stimulus in trials
with an \unit[8]{s} inter-stimulus delay. Consequently, each run consisted of
25 trials, and no trials were discarded from the analysis.  There was no
additional fixation at the start of a run. For further details, see
\citet{HDH+2015}, and \citet{HBI+14} for details on MRI acquisition methods.


\subsection*{Preprocessing}

Approximate temporal lobe masks for each participant were extracted from
Montreal Neurological Institute coordinate space using FSL
\citep{SJB+04,JBB+12}, and projected into the subject-specific coordinate
system.  Each voxel inside the temporal lobe mask was run-wise $Z$-scored and
linearly de-trended using PyMVPA \citep{HHS09b}. 

\subsection*{Encoding model}

To build an encoding model with high predictive power, we need to find an
appropriate feature representation of the music stimuli.  \citet{CTK+2012}
already compared different feature representations of the same stimuli in the
3-Tesla dataset. They found features corresponding to the timbre of the
stimulus
% MIH: we haven't introduced this metric yet
%showed the highest binary retrieval accuracy.
offer the best discriminative power.  We chose a similar feature set made
available by \citet{HDH+2015}, the low-quefrency mel-frequency spectrum
(LQ-MFS) of the stimulus.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{pics/encoding_scheme}

  \caption{A schematic overview of the encoding process. The spectrogram for
    each stimulus is transformed into its low-quefrency mel-frequency spectrum
    (LQ-MFS). Then, the encoding features are extracted by a sliding window
    from the LQ-MFS. Using these features, encoding model is trained on all
    runs in the training set, and used to predict the BOLD activity of the
  left-out run.  These predictions are subsequently used for validation.}

 \label{fig:encoding_scheme}
\end{figure}


For each f{MRI} sample $y_{vt}$ (where $t=1,2,..,T$ denotes the time-points and
$v=1,2,..,V$ denotes the voxels) the LQ-MFS features $x_{t}$
$[1\times\widetilde{M}]$ (where $\widetilde{M}$ is the number of LQ-MFS
coefficients) of the corresponding\todo{the 2s right before the acquisition timepoint?} two second part of the stimulus were
computed. In case there was no stimulus presented at time-point $t$, a zero
vector $[1\times\widetilde{M}]$ was used. 

As the BOLD response is delayed,  the most recent feature vector was removed
for each f{MRI} sample (corresponds to an assumed \unit[2]{s} stimulus-response
delay), and the new feature vector at time-point $t$ was created by
concatenating the prior feature vectors $x_{t-1}$,$x_{t-2}$ and $x_{t-3}$ (see
Figure \ref{fig:encoding_scheme}). From now on, we denote this stacked feature
vector as $x_{t}$.  Feature vectors (and the corresponding f{MRI} sample) were
removed from the analysis, if two-thirds or more of the concatenated feature
vectors were zero-vectors.

The BOLD activity time-series, as well as the feature time-series, were
vertically stacked, resulting in a matrix of features $X$ $[N\times M]$ (where
$N$ is the number of f{MRI} samples, and $M$ is number of LQ-MFS coefficients,
with $M=3\widetilde{M}$\todo{obvious what it means, but still looks odd}) and a matrix of BOLD activity $Y$ $[N\times V]$ (where
$V$ is the number of voxels).

This lagging of the stimulus allows us to train the encoding model to predict
the f{MRI} time-series without explicitly modelling the BOLD response.

%probabilistic or objective function minimization, also cite someone about
%using lin. reg. in encoding
The encoding model could then be expressed as the probability to observe the
BOLD activity at time-point $t$ and voxel $v$:
%
\begin{equation}
  \label{eq:encmo}
  p(y_{vt}|x_{t}) = N(y_{vt};x_{t}\beta_{v},\sigma)
\end{equation}
%
where $N(y;\mu,\sigma)$ denotes the probability density at point $y$ for a
Gaussian with mean $\mu$ and standard deviation $\sigma$, and $\beta_{v}$ is a
$[M\times1]$ vector of regression coefficients specific to voxel $v$. To reduce
over-fitting, the regression-coefficients were estimated using ridge regression
\citep{HK70}.  Independently for each voxel, the regularization parameter
$\lambda$ with the lowest mean squared error in a generalized leave-one-out
cross-validation \citep{GHW79} was chosen from a set of candidate values.
\todo{arbitrary selection?}

\subsection*{Quality metrics} 

\subsubsection*{Binary retrieval accuracy}

Binary retrieval accuracy \citep{ML08} tests if an encoding model's predictions
can differentiate a stimulus' observed BOLD activity from the BOLD activity of
a decoy stimulus.  Specifically, a stimulus pair is counted as succesfully
classified, if the cosine similarity between predicted and observed f{MRI}
responses is greater for the correctly matched predictions and observations
than for the incorrectly matched ones (i.e. the similarity of the observed
response of stimulus A with the predictions for stimulus B and vice versa).
The correct matches for all pair-wise combinations of stimuli, including
stimuli from the same genre, are counted and then divided by the number of
combinations.\todo{one pair per stimulus, or multiple random pairs?}

\subsubsection*{Matching score}
%
An alternative measure of encoding performance is the correlation rank score or
matching score \citep{SF14}. For each stimulus label $l_{n}$ in the validation
set, its predicted BOLD activity $\widetilde{y}_{n}$ is correlated with the
observed BOLD activity of every stimulus, $y_{i}$ for $i=1..25$. These
correlations are then ordered, and the  matching score $m(l_{n})$ is \[
m(l_{n}) = 1-\frac{rank(l_{n})-1}{N-1} \] Where $rank(l_{n})$ is the rank of
the correlation between predicted $\widetilde{y}_{n}$ and observed $y_{n}$ BOLD
activity of $l_{n}$. Finally, the matching scores for all stimuli in the
validation set are averaged.



\begin{figure}
  \centering
  \includegraphics[width=9cm]{pics/Decoding_scheme}

  \caption{A schematic overview of the decoding of music category. The
    predicted f{MRI} time series of the validation run is reduced in
    dimensionality by principal component analysis, and a multivariate-normal
    likelihood function $p(y_{t}|x_{t})$ is constructed.  From there, the
  probability distribution over music stimuli and subsequently musical
categories is estimated.
\todo[inline]{this is talking about a 2-part decoding scheme. would be good to
  label those parts. Probably best to include a symbolic reference that the
  basic scores (without decoding) are computed from the predicted time series
  at the top. The manuscript is unclear how models from multiple voxels are
treated in this case. Is the correlation distance computed for all timepoints
of a stimulus across all the voxels? or per voxel and averaged?}
}

 \label{fig:decoding_scheme}
\end{figure}


\subsubsection*{Decoding}

Instead of testing the encoding performance, we can also test the performance
of a decoder based on the individual encoding models \citep{NG11} (Figure
\ref{fig:decoding_scheme}).

To go from the probability of a voxel activation
given stimulus features $p(y_{vt}|x_{t})$ to probability of stimulus features
given voxel activation $p(x_{t}|y_{t})$ we follow \citet{NG09} and separate our
decoding scheme into two parts. 

\paragraph{Single- to Multi-Voxel Encoding}

First we condense the large number of voxel-specific encoding models into one
multi-voxel encoding model.  To do this we project the predicted and observed
f{MRI} data onto the first $k$ principal components of the $[N\times V]$ matrix
of predicted BOLD activity, where $k$ is the number of principal components
that maximize the decoding accuracy on the training set for this participant.
As in \citet{NG09} we construct the $k$-dimensional multivariate normal
probability density function $p(y_{t}|x_{t})$ to obtain a likelihood function
across voxels. 

\todo[inline]{also for the other two metrics?}

\paragraph{Multi-Voxel Encoding to Decoding}

We now express this likelihood in terms of the label of the music stimulus, instead of its LQ-MFS features.  We
use the simplifying assumption that the BOLD activity is influenced by the
music stimuli only through their LQ-MFS coefficients $x$, and --- given that
each music stimulus was associated with only three (lagged) LQ-MFS
representations --- the likelihood to observe a given triple of consecutive $y$
for a specific music stimulus $l_{i}$ is $p(y|l_{i}) \propto
p(y_{t-1}|x_{1})p(y_{t}|x_{2})p(y_{t+1}|x_{3})$ where $t$ is the sample 6
seconds after the start of the music stimulus and $x$ are the three LQ-MFS
feature vectors of this stimulus.  For a given triple of consecutive BOLD
activity $y$ from the same stimulus, we can now estimate the probability
distribution over music stimuli $p(l_{i}|y)$ (for $i=1..25$) by using Bayes'
rule: $p(l_{i}|y) \propto p(y|l_{i})p(l_{i})$.
Since each stimulus was presented was presented exactly once,
it has an uniform prior distribution with $p(l_{i})=\frac{1}{25}$. The mode of
this distribution is the most probable presented stimulus given the data. 
Additionally we can decode the musical genre of the presented stimulus given the observed
BOLD activity as the mode of $p(c_{i}|y) = \sum\nolimits_{l \in
  stim(c_{i})} p(l|y)$ where $stim(c_{i})$ are the labels of the stimuli
  belonging to the genre $c_{i}$. 


\subsection*{Voxel selection}

We varied the number of voxels used in the analysis, both for 3- and 7-Tesla
f{MRI} data, and selected which voxels to keep by two different criteria. Both
criteria were based on voxel characteristics in the training set only.

\paragraph{Selection by stability}

\citet{ML08} selected the 500 most stable voxels for their analysis. For a
single voxel, each run can be represented as a vector of BOLD activity, where
each entry is associated with one stimulus. A voxel's "stability score" is then
the average (pair-wise) correlation between the vectors of the eight runs for
all combinations of runs.  This criterion selects voxels with consistent
activation for each stimulus across runs.

\paragraph{Selection by $r^2$}

As we are interested in encoding performance, we can use the quality of
predictions of each voxel's encoding model as a selection criterion. We compute
the coefficient of determination $r^2$ for each voxel-specific encoding model
in the training set. Using this criterion selects voxels whose activity can be
explained best by an LQ-MFS-based encoding model.

\section*{Results}

We varied the number of voxels used in the analysis and how they were selected,
both for 3- and 7-Tesla f{MRI} data, and compared the resulting differences in
three encoding metrics. To differentiate between effects of voxel number and
overall volume of the voxels, which differs in 3- and 7-Tesla, we show the
results as a function of the number of voxels, as well as overall volume of
these voxels.

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/binary_score_joint.pdf_tex}
	
  \caption{\textbf{A} Mean binary retrieval accuracy as a function of the
  included number of voxels for 3- and 7-Tesla, for stability- and $r^2$-based
  voxel selection. Error bars denote the bootstrapped 95\% confidence interval
  of the mean. The mean is taken over binary retrieval accuracies of eight runs
  for each of the 19 participants. \textbf{B} Mean binary retrieval accuracy as
a function of the overall volume of the included voxels for 3- and 7-Tesla, for
stability- and $r^2$-based voxel selection.
\todo[inline]{250 r2 features are best, regardless of how much they sample. but
it 27cm3 of stability give best perf. which shape would the accuracy curve have?}}

 \label{fig:binary_retrieval}
\end{figure}

\subsection*{Binary retrieval accuracy}

Differences in binary retrieval accuracy are shown in Figure
\ref{fig:binary_retrieval}. For both selection criteria, binary retrieval
accuracy is higher in 3-Tesla than 7-Tesla data for lower numbers of voxels.
When a higher number of the most stable voxels are included, binary retrieval
accuracy increases for 7-Tesla data, while decreasing for 3-Tesla data.    In
contrast, increasing the number of voxels included by $r^2$ decreases the
binary retrieval accuracy for 3-Tesla, as well as 7-Tesla data, although the
decrease seen in 3-Tesla data is larger. The peak performance of 3-Tesla as
well as 7-Tesla encoding models, is higher using voxels selected by their
$r^2$.  Indexing by the overall volume of the voxels, reveals a very similar,
but shifted, pattern for 3- and 7-Tesla data.
\todo{differences for r2 selection seem within the margin of uncertainty, do we need a test?}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/matching_score_joint.pdf_tex}
	
  \caption{\textbf{A} Mean matching score as a function of the included number
  of voxels for 3- and 7-Tesla, for stability- and $r^2$-based voxel selection.
  Error bars denote the bootstrapped 95\% confidence interval of the mean. The
  mean is taken over binary retrieval accuracies of eight runs for each of the
  19 participants. \textbf{B} Mean matching score as a function of the overall
volume of the included voxels for 3- and 7-Tesla, for stability- and
$r^2$-based voxel selection.
\todo[inline]{27cm3 is optimal, regardless of \#voxels}}

 \label{fig:matching_score}
\end{figure}

\subsection*{Matching score}

In Figure \ref{fig:matching_score}, the matching score is shown. Again, for few
voxels, encoding models for 3-Tesla data produce a higher score than encoding
models for 7-Tesla data. Increasing the number of voxels increases the matching
score for 7-Tesla data, and decreases the score for 3-Tesla data. This pattern
is present in both selection criteria. For voxels selected by $r^2$, there is a
steeper increase and decrease and a higher peak for both field strengths.  If
indexed by the overall voxel volume, similarities in the overlapping regions of
the 3- and 7-Tesla matching score curves can be seen.\todo{reason $\approx$27cm3 actually carry useful signal?}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/binary_score_joint_mve.pdf_tex}
	
  \caption{\textbf{A} Mean binary retrieval accuracy for a single encoding
  model as a function of the included number of voxels for 3- and 7-Tesla, for
  stability- and $r^2$-based voxel selection. Error bars denote the
  bootstrapped 95\% confidence interval of the mean. The mean is taken over
  binary retrieval accuracies of eight runs for each of the 19 participants.
  \textbf{B} Mean binary retrieval accuracy for a single encoding model as a
  function of the overall volume of the included voxels for 3- and 7-Tesla, for
stability- and $r^2$-based voxel selection.
\todo[inline]{this figure is practically identical to Fig3}}

\label{fig:binary_retrieval_mve}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/matching_score_joint_mve.pdf_tex}
	
  \caption{\textbf{A} Mean matching score for a single encoding model as a
  function of the included number of voxels for 3- and 7-Tesla, for stability-
  and $r^2$-based voxel selection. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over binary retrieval
  accuracies of eight runs for each of the 19 participants. \textbf{B} Mean
matching score for a single encoding model as a function of the overall volume
of the included voxels for 3- and 7-Tesla, for stability- and $r^2$-based voxel
selection.
\todo[inline]{this figure is now also practically identical to Fig3, hence the
differences in the scores are eliminated (except for magnitude) by
dimensionality reduction. Not immediately clear to me why this would happen\ldots}}

 \label{fig:matching_score_mve}
\end{figure}


\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/Binary_score_mve_2.pdf_tex}
	
  \caption{Like \ref{fig:binary_retrieval_mve} but with reduced encoding model that retains 95\% of
  variance}

 \label{fig:binary_retrieval_mve_2}
\end{figure}


\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/Matching_score_mve_2.pdf_tex}
	
  \caption{Like \ref{fig:matching_score_mve} but with reduced encoding model that retains 95\% of
  variance}

 \label{fig:matching_score_mve_2}
\end{figure}


\subsection*{Effect of Multi-Voxel Encoding}

To invert the voxel-wise encoding models \citet{NG11} first reduce the
dimensionality of the predicted activity patterns. Differences between the
decoding accuracy and the two encoding metrics might be caused by either this
dimensionality reduction or the inversion of the encoding model. To separate
these causes, we estimate the binary retrieval accuracy and the matching score
using the same reduced encoding model as described for the decoding.

Evaluating a reduced encoding model with binary retrieval accuracy (Figure
\ref{fig:binary_retrieval_mve}) produces a pattern similar to the non-reduced
form. The matching score (Figure \ref{fig:matching_score_mve}) on the other hand
changes its pattern with a reduced encoding model, getting more similar to the
pattern of the binary retrieval accuracy.

Instead of choosing the dimensionality of the encoding model so that it
maximizes the decoding accuracy, we can also reduce the model to the number of dimensions
that still retain a fixed amount of variance. The binary retrieval accuracy and
the matching score for a Multi-Voxel Encoding model that retains 95\% of the variance is shown in
Figure \ref{fig:binary_retrieval_mve_2} and \ref{fig:matching_score_mve_2}.

\todo[inline]{I'm unhappy about what the next paragraph + plot explains vs. the
added complexity of looking "into" the matching score. Remove it and just state
that Binary retrieval acc. is less sensitive to changes than matching score
(maybe because it quickly saturates)?}

Both encoding metrics use a similarity measure between predicted and observed
f{MRI} activity patterns, the correlation coefficient and the cosine similarity, but only
one is affected by a dimensionality reduction of the encoding model. To explain
this difference, we directly compare the effect of a reduced encoding model on
the correlation and cosine similarity between predicted and observed f{MRI}.
All voxels in this analysis were selected by $r^2$.
We compute the 80th percentile of the correlations/cosine similarities between
the predicted activity of one stimulus with the observed activity of all other
stimuli, we can compare this with the correlation/cosine similarity between the
predicted and observed activity associated with this stimulus. 
If the correlation between observed and predicted f{MRI} activity is equal to
the 80th percentile, the matching score of this stimulus would equal $0.8$,
close to the observed values.

The pattern present in the matching score can also be seen in the top row of Figure
\ref{fig:corr_cossim_differences}, the gap between the blue and green curve
widens in 3T for lower numbers of voxels, and in 7T for higher numbers of
voxels. Reducing the dimensionality of the encoding model improves the
correlation of predicted and observed stimulus activity -- the dashed red line
-- but also increases the correlation with the activity of all other stimuli --
the dashed purple line --, leading to a gap between the two curves and a net
decrease in the matching score.

Cosine similarity and correlation change substantially when reducing the
dimensionality of the encoding models. Yet, this affects only the matching score, not the binary
retrieval accuracy (compare Figure \ref{fig:binary_retrieval} and
\ref{fig:binary_retrieval_mve}). This change in the cosine similarity of the predicted
stimulus activity with observed as well as unrelated activity, does not alter
the binary retrieval accuracy.


\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/MVE_fs_similarity.pdf_tex}
	
  \caption{Similarity between the predicted f{MRI} activity for a stimulus and
  this stimulus' observed activity (self) or the observed activity of all other
  stimuli (other), averaged across participants, runs and stimuli. The blue and purple
  lines are the means of the 80th-percentiles of either the correlation (top row) or the
  cosine similarity (bottom row) of the predicted activity of each stimulus with the observed
  activity of all other stimuli.}

 \label{fig:corr_cossim_differences}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/Stimulus_decoding.pdf_tex}
	
  \caption{ Decoding of the stimulus itself with dimensionality chosen so it
    retains 95\% of variance \todo[inline]{extend y-axis to include 0.04 as chance level?} }

 \label{fig:decoding_accuracy_stimulus}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/Category_decoding_cv.pdf_tex}
	
  \caption{\textbf{A} Mean decoding accuracy of music category as a function of
  the included number of voxels for 3- and 7-Tesla, for stability- and
  $r^2$-based voxel selection. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over decoding
  accuracies of eight runs for each of the 19 participants. \textbf{B} Mean
decoding accuracy of music category as a function of the overall volume of the
included voxels for 3- and 7-Tesla, for stability- and $r^2$-based voxel
selection.
\todo[inline]{fewer voxels needed for genre classification? is that because it is simpler?}
}

 \label{fig:decoding_accuracy}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/Category_decoding_var95.pdf_tex}
	
  \caption{ Decoding accuracy now with the number of eigenvectors chosen to
  retain 95\% of the variance instead of cross-validated for decoding }

 \label{fig:decoding_accuracy_var95}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/cat_decoding_99var.pdf_tex}
	
  \caption{ Decoding accuracy for selection by $r^2$, with the number of eigenvectors chosen to
	  retain 99\% of the variance (without SVM). }

 \label{fig:decoding_accuracy_var99}
\end{figure}


\subsection*{Decoding accuracy}

The accuracy of decoding which stimulus was presented
is shown in Figure \ref{fig:decoding_accuracy_stimulus}. %description here

The decoding accuracy of each stimulus's music category is shown in Figure
\ref{fig:decoding_accuracy}. As the decoder is based on an encoding model, a
natural comparison is with the performance of a standard decoder. For both
selection methods, we contrast the decoding performance of the encoding model
with the decoding performance of a linear Support Vector Machine (SVM)
\citep{FCH+08,V13}. While such a decoder can use the subset of the most stable
voxels, it can only select the voxels with the highest $r^2$, if it leverages
the information of an encoding model. We still include these data, to show
which information could be extracted from these voxels by a more sophisticated
decoding approach.  The decoding accuracy was higher for 7-Tesla data \todo{fig 10?} than for
3-Tesla data, across all voxel numbers and selection criteria.  Selecting the
most stable voxels, the decoding accuracy slightly increases for 7-Tesla data
with higher numbers of voxels, while decreasing for 3-Tesla data.  If we select
the voxel-specific encoding models with the highest $r^2$, the decoding
accuracy decreases for 3-Tesla data, but first stays level and then decreases
for 7-Tesla data. Encoding models on 3-Tesla data show a higher or equal
decoding accuracy than a discriminative decoder across all numbers of voxels.
This contrasts the pattern present in the 7-Tesla data, where our decoding
model shows a higher decoding accuracy only for smaller numbers of voxels.

Instead of choosing the dimensionality of the multi-voxel encoding model to
maximize the genre decoding accuracy, we can also choose the dimensionality that retains
95\% of the variance (Figure \ref{fig:decoding_accuracy_var95}). A higher field
strength still provides a better decoding accuracy, but not optimizing the
dimensionality for decoding reduces the accuracy slightly, and shifts the peak
accuracy to a higher number of voxels. For a different percentage of retained
variance, we see a different pattern: Retaining 99\% of variance strongly
reduces the decoding accuracy in 7T for a high number of voxels. 
%bc of much higher dimensionality than when CVed

Reducing the dimensionality of the encoding model is necessary for stabilising
the covariance matrix of the multivariate normal likelihood \citet{NG11}.  
Since the decoding accuracy varies with the dimensionality of the reduced
encoding model, we were interested if a small number of voxels might give a
similar performance without the need for reducing the encoding model. As in the
reduced encoding model, we constructed the likelihood out of the residuals of
the predicted f{MRI} activity. In Figure \ref{fig:decoding_full} we show the
accuracy of such a decoding model for up to 1000 voxels, the overlap with the
range of the reduced encoding model is intentional, to see where -- or if -- not
reducing the encoding models results in worse performance.

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/decoding_full_few_vxl.pdf_tex}
	
  \caption{  Mean decoding accuracy of music category using a full encoding
	  model for 3- and 7-Tesla, for stability- and
  $r^2$-based voxel selection, represented as a function of the number of voxels
  or their volume. Error bars denote the bootstrapped 95\%
  confidence interval of the mean. The mean is taken over decoding
  accuracies of eight runs for each of the 19 participants.
 }

 \label{fig:decoding_full}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/r2_means.pdf_tex}
	
  \caption{ Mean $r^2$ for the voxels with highest $r^2$.}

 \label{fig:r2}
\end{figure}

\begin{figure}
  \centering
  \def\svgwidth{\linewidth}
  \input{pics/Mean_fstatistic_stim_cat_joint.pdf_tex}
	
  \caption{ Mean F-statistic for the f{MRI} time series (left column) and the
  predicted voxel activity (right column) for genre or stimulus.
  \todo[inline]{Alternatives: with MANOVA instead of anova, check for dimensionality reduced model (is there a
  decrease or increase?), check for most stable voxels (does an encoding model
  increase the discriminability there too?), or select voxels for auditory cortex/STS)}}

 \label{fig:anova}
\end{figure}

\subsection*{Field strength-specific differences}

Field strength affects performance differentially in the quality
metrics used.
Encoding models trained on 3T outperform encoding models trained on 7T data for
low number of voxels in binary retrieval accuracy and the matching score, while
the effect is reversed for a  high number of voxels. Yet, the same 3T encoding models
perform consistently worse than 7T-trained models in decoding the music genre.
This difference persists even when the dimensionality of the encoding models is
reduced. 

Unique predictions for a given stimulus do not need to align with genre
classification, an encoding model that predicts stimulus-specific data well
might not be sensitive to parts of the stimulus that are specific to its genre. 
%etc

To quantify each voxel's encoding performance -- in terms of explained
variance -- with the discriminability of the selected voxels, we order each
participant's voxels by explained variance and compute the mean $r^2$ for each
voxel across participants (Figure \ref{fig:r2}). 

To test for the discriminability of a voxel, we can also compute its F-statistic
for a grouping by stimulus or music genre. This measure is the between-group --
either for genre or individual stimuli -- variability divided by the within-group
variability. We can then compare if an encoding model actually increases the
discriminability, by computing this for the original f{MRI} time series as well
as predicted time series. In Figure \ref{fig:anova} we show the F-statistic first averaged over the
voxels selected by $r^2$ and then averaged over participants, for the original
time series (left column) and for the time series predicted by an encoding model
(right column).


\section*{Discussion}

%MIH: what if less than 250 voxels?
% Fig 3: why only 100 good voxels in both 3T and 7T?

Our results show considerable variation of different quality metrics with
parameters of the data analysis and BOLD-acquisition. We demonstrate that the
patterns of this variation differ between validation strategies and that the
effects of field strength, voxel selection, and voxel number are inconsistent
across these metrics.\todo{seems feature extraction has more impact than
plain voxel selection}

Specifically, we show a similar performance in purely encoding based metrics
for 3- and 7-Tesla data with a slightly worse peak performance for 7-Tesla, but
observe that this effect is mediated by the number of voxels used. A previous
study \citep{SF14} found a better performance for 7-Tesla data for the matching
score, although using a different number of stimuli in the 3- and 7-Tesla
datasets. %Furthermore, this study differs in the selection of voxels, the
%choice of stimulus features and the use
%of an explicit BOLD-model instead of a lagged feature representation.

The number of voxels affect the encoding performance differentially for low and
high field strength. This is not surprising, as voxel sizes differ for these
particular 3- and 7-Tesla data. Indexing by the overall volume of the selected
voxels, instead of their number, reveals similarities in performance when 3-
and 7-Tesla encoding models are trained on a similar volume. 
\todo[inline]{true, but I am struggeling to find a compact description for this when
  considering the reduced model. Argg, hold on! If we select 250 features
  of the reduced model, we cannot compute a corresponding volume, as they
will be all over the place!!
so I guess the lower rows of fig 6 and 7, possibly fig 5 need to be removed.}

Selecting the voxels by $r^2$ instead of a stability criterion, improves
performance especially in the lower number of voxels for the binary retrieval
accuracy and the decoding accuracy, while widening the performance gap between
3- and 7-Tesla data in the matching score. Including voxels ordered by $r^2$
--- a measure that captures encoding performance in a single voxel --- gives
diminishing returns for higher numbers of voxels: Voxels for which an encoding
model performs well are already included, and each additional voxel decreases
the joint encoding performance.  Interestingly, this pattern is not seen in the
matching score, where performance in 7-Tesla is worse for the smallest number
of voxels and increases with voxel number. The same pattern is seen in the
performance of the linear SVM. Even with information from an encoding model ---
selecting voxels by $r^2$ --- a standard decoder can initially not take
advantage of this information as well as an encoding model. For the high field
strength condition, the decoder improves if more voxels are included and
eventually performs better than the encoding model. This might be an effect of
the sheer number of features used by the decoder, or of the information content
of these voxels, i.e. because the decoder profits from pattern information that
is not related to the LQ-MFS features of the stimulus, and would therefore be
more present in a voxel with lower $r^2$ in encoding LQ-MFS.


\section*{Author contributions}
%In order to give appropriate credit to each author of an article, the
%individual contributions of each author to the manuscript should be detailed
%in this section. We recommend using author initials and then stating briefly
%how they contributed.

MB performed the analysis and wrote the manuscript.
JSG contributed to the manuscript.
MH contributed to the manuscript.
\todo{some authors are missing here}


\section*{Competing Interests}

No competing interests were disclosed.

\section*{Grant Information}

This research was, in part, supported by the German Federal Ministry of
Education and Research (BMBF) as part of a US-German collaboration in
computational neuroscience (CRCNS; awarded to James Haxby, Peter Ramadge, and
Michael Hanke), co-funded by the BMBF and the US National Science Foundation
(BMBF 01GQ1112; NSF 1129855).  Work on the data-sharing technology employed for
this research was supported by US-German CRCNS project awarded to
Yaroslav~O.~Halchenko and Michael~Hanke, co-funded by the BMBF and the US
National Science Foundation (BMBF 01GQ1411; NSF 1429999).  Michael Hanke was
supported by funds from the German federal state of Saxony-Anhalt, Project:
Center for Behavioral Brain Sciences.


\section*{Acknowledgements}
%This section should acknowledge anyone who contributed to the research or the
%article but who does not qualify as an author based on the criteria provided
%earlier (e.g. someone or an organisation that provided writing assistance).
%Please state how they contributed; authors should obtain permission to
%acknowledge from all those mentioned in the Acknowledgements section.  Please
%do not list grant funding in this section (this should be included in the
%Grant information section - See above).

We are grateful to Michael Casey and the musicians ;) \ldots
\todo{move Michael into the author list, if he agrees}

\todo[inline]{express gratitude}

\bibliography{references}
\end{document}

% vim: textwidth=80 colorcolumn=81
